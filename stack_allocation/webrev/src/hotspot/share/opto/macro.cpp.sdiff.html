<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Sdiff src/hotspot/share/opto/macro.cpp</title>
    <link rel="stylesheet" href="../../../../style.css" />
  </head>
<body>
<center><a href="machnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.hpp.sdiff.html" target="_top">next &gt;</a></center>    <h2>src/hotspot/share/opto/macro.cpp</h2>
     <a class="print" href="javascript:print()">Print this page</a>
<table>
<tr valign="top">
<td>
<hr />
<pre>
 286         mem = in-&gt;in(TypeFunc::Memory);
 287       } else {
 288         assert(false, &quot;unexpected projection&quot;);
 289       }
 290     } else if (mem-&gt;is_Store()) {
 291       const TypePtr* atype = mem-&gt;as_Store()-&gt;adr_type();
 292       int adr_idx = phase-&gt;C-&gt;get_alias_index(atype);
 293       if (adr_idx == alias_idx) {
 294         assert(atype-&gt;isa_oopptr(), &quot;address type must be oopptr&quot;);
 295         int adr_offset = atype-&gt;offset();
 296         uint adr_iid = atype-&gt;is_oopptr()-&gt;instance_id();
 297         // Array elements references have the same alias_idx
 298         // but different offset and different instance_id.
 299         if (adr_offset == offset &amp;&amp; adr_iid == alloc-&gt;_idx)
 300           return mem;
 301       } else {
 302         assert(adr_idx == Compile::AliasIdxRaw, &quot;address must match or be raw&quot;);
 303       }
 304       mem = mem-&gt;in(MemNode::Memory);
 305     } else if (mem-&gt;is_ClearArray()) {







 306       if (!ClearArrayNode::step_through(&amp;mem, alloc-&gt;_idx, phase)) {
 307         // Can not bypass initialization of the instance
 308         // we are looking.
 309         debug_only(intptr_t offset;)
 310         assert(alloc == AllocateNode::Ideal_allocation(mem-&gt;in(3), phase, offset), &quot;sanity&quot;);
 311         InitializeNode* init = alloc-&gt;as_Allocate()-&gt;initialization();
 312         // We are looking for stored value, return Initialize node
 313         // or memory edge from Allocate node.
 314         if (init != NULL)
 315           return init;
 316         else
 317           return alloc-&gt;in(TypeFunc::Memory); // It will produce zero value (see callers).
 318       }
 319       // Otherwise skip it (the call updated &#39;mem&#39; value).
 320     } else if (mem-&gt;Opcode() == Op_SCMemProj) {
 321       mem = mem-&gt;in(0);
 322       Node* adr = NULL;
 323       if (mem-&gt;is_LoadStore()) {
 324         adr = mem-&gt;in(MemNode::Address);
 325       } else {
</pre>
<hr />
<pre>
 710           if (use-&gt;outcnt() == 1 &amp;&amp; use-&gt;unique_out()-&gt;Opcode() == Op_Return) {
 711             NOT_PRODUCT(fail_eliminate = &quot;Object is return value&quot;;)
 712           } else {
 713             NOT_PRODUCT(fail_eliminate = &quot;Object is referenced by Phi&quot;;)
 714           }
 715           DEBUG_ONLY(disq_node = use;)
 716         } else {
 717           if (use-&gt;Opcode() == Op_Return) {
 718             NOT_PRODUCT(fail_eliminate = &quot;Object is return value&quot;;)
 719           }else {
 720             NOT_PRODUCT(fail_eliminate = &quot;Object is referenced by node&quot;;)
 721           }
 722           DEBUG_ONLY(disq_node = use;)
 723         }
 724         can_eliminate = false;
 725       }
 726     }
 727   }
 728 
 729 #ifndef PRODUCT
<span class="line-modified"> 730   if (PrintEliminateAllocations) {</span>
 731     if (can_eliminate) {
 732       tty-&gt;print(&quot;Scalar &quot;);
 733       if (res == NULL)
 734         alloc-&gt;dump();
 735       else
 736         res-&gt;dump();
 737     } else if (alloc-&gt;_is_scalar_replaceable) {
 738       tty-&gt;print(&quot;NotScalar (%s)&quot;, fail_eliminate);
 739       if (res == NULL)
 740         alloc-&gt;dump();
 741       else
 742         res-&gt;dump();
 743 #ifdef ASSERT
 744       if (disq_node != NULL) {
 745           tty-&gt;print(&quot;  &gt;&gt;&gt;&gt; &quot;);
 746           disq_node-&gt;dump();
 747       }
 748 #endif /*ASSERT*/
 749     }
 750   }
 751 #endif
 752   return can_eliminate;
 753 }
 754 












 755 // Do scalar replacement.
 756 bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray &lt;SafePointNode *&gt;&amp; safepoints) {
 757   GrowableArray &lt;SafePointNode *&gt; safepoints_done;
 758 
 759   ciKlass* klass = NULL;
 760   ciInstanceKlass* iklass = NULL;
 761   int nfields = 0;
 762   int array_base = 0;
 763   int element_size = 0;
 764   BasicType basic_elem_type = T_ILLEGAL;
 765   ciType* elem_type = NULL;
 766 
 767   Node* res = alloc-&gt;result_cast();
 768   assert(res == NULL || res-&gt;is_CheckCastPP(), &quot;unexpected AllocateNode result&quot;);
 769   const TypeOopPtr* res_type = NULL;
 770   if (res != NULL) { // Could be NULL when there are no users
 771     res_type = _igvn.type(res)-&gt;isa_oopptr();
 772   }
 773 
 774   if (res != NULL) {
</pre>
<hr />
<pre>
 867           }
 868           JVMState *jvms = sfpt_done-&gt;jvms();
 869           jvms-&gt;set_endoff(sfpt_done-&gt;req());
 870           // Now make a pass over the debug information replacing any references
 871           // to SafePointScalarObjectNode with the allocated object.
 872           int start = jvms-&gt;debug_start();
 873           int end   = jvms-&gt;debug_end();
 874           for (int i = start; i &lt; end; i++) {
 875             if (sfpt_done-&gt;in(i)-&gt;is_SafePointScalarObject()) {
 876               SafePointScalarObjectNode* scobj = sfpt_done-&gt;in(i)-&gt;as_SafePointScalarObject();
 877               if (scobj-&gt;first_index(jvms) == sfpt_done-&gt;req() &amp;&amp;
 878                   scobj-&gt;n_fields() == (uint)nfields) {
 879                 assert(scobj-&gt;alloc() == alloc, &quot;sanity&quot;);
 880                 sfpt_done-&gt;set_req(i, res);
 881               }
 882             }
 883           }
 884           _igvn._worklist.push(sfpt_done);
 885         }
 886 #ifndef PRODUCT
<span class="line-modified"> 887         if (PrintEliminateAllocations) {</span>
 888           if (field != NULL) {
 889             tty-&gt;print(&quot;=== At SafePoint node %d can&#39;t find value of Field: &quot;,
 890                        sfpt-&gt;_idx);
 891             field-&gt;print();
 892             int field_idx = C-&gt;get_alias_index(field_addr_type);
 893             tty-&gt;print(&quot; (alias_idx=%d)&quot;, field_idx);
 894           } else { // Array&#39;s element
 895             tty-&gt;print(&quot;=== At SafePoint node %d can&#39;t find value of array element [%d]&quot;,
 896                        sfpt-&gt;_idx, j);
 897           }
 898           tty-&gt;print(&quot;, which prevents elimination of: &quot;);
 899           if (res == NULL)
 900             alloc-&gt;dump();
 901           else
 902             res-&gt;dump();
 903         }
 904 #endif
 905         return false;
 906       }
 907       if (UseCompressedOops &amp;&amp; field_type-&gt;isa_narrowoop()) {
 908         // Enable &quot;DecodeN(EncodeP(Allocate)) --&gt; Allocate&quot; transformation
 909         // to be able scalar replace the allocation.
 910         if (field_val-&gt;is_EncodeP()) {
 911           field_val = field_val-&gt;in(1);
 912         } else {
 913           field_val = transform_later(new DecodeNNode(field_val, field_val-&gt;get_ptr_type()));
 914         }
 915       }
 916       sfpt-&gt;add_req(field_val);
 917     }
<span class="line-modified"> 918     JVMState *jvms = sfpt-&gt;jvms();</span>
<span class="line-removed"> 919     jvms-&gt;set_endoff(sfpt-&gt;req());</span>
<span class="line-removed"> 920     // Now make a pass over the debug information replacing any references</span>
<span class="line-removed"> 921     // to the allocated object with &quot;sobj&quot;</span>
<span class="line-removed"> 922     int start = jvms-&gt;debug_start();</span>
<span class="line-removed"> 923     int end   = jvms-&gt;debug_end();</span>
<span class="line-removed"> 924     sfpt-&gt;replace_edges_in_range(res, sobj, start, end);</span>
<span class="line-removed"> 925     _igvn._worklist.push(sfpt);</span>
 926     safepoints_done.append_if_missing(sfpt); // keep it for rollback
 927   }
 928   return true;
 929 }
 930 
 931 static void disconnect_projections(MultiNode* n, PhaseIterGVN&amp; igvn) {
 932   Node* ctl_proj = n-&gt;proj_out_or_null(TypeFunc::Control);
 933   Node* mem_proj = n-&gt;proj_out_or_null(TypeFunc::Memory);
 934   if (ctl_proj != NULL) {
 935     igvn.replace_node(ctl_proj, n-&gt;in(0));
 936   }
 937   if (mem_proj != NULL) {
 938     igvn.replace_node(mem_proj, n-&gt;in(TypeFunc::Memory));
 939   }
 940 }
 941 
 942 // Process users of eliminated allocation.
 943 void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {
 944   Node* res = alloc-&gt;result_cast();
 945   if (res != NULL) {
</pre>
<hr />
<pre>
1001           // Disconnect src right away: it can help find new
1002           // opportunities for allocation elimination
1003           Node* src = ac-&gt;in(ArrayCopyNode::Src);
1004           ac-&gt;replace_edge(src, top());
1005           // src can be top at this point if src and dest of the
1006           // arraycopy were the same
1007           if (src-&gt;outcnt() == 0 &amp;&amp; !src-&gt;is_top()) {
1008             _igvn.remove_dead_node(src);
1009           }
1010         }
1011         _igvn._worklist.push(ac);
1012       } else {
1013         eliminate_gc_barrier(use);
1014       }
1015       j -= (oc1 - res-&gt;outcnt());
1016     }
1017     assert(res-&gt;outcnt() == 0, &quot;all uses of allocated objects must be deleted&quot;);
1018     _igvn.remove_dead_node(res);
1019   }
1020 




1021   //
1022   // Process other users of allocation&#39;s projections
1023   //
1024   if (_resproj != NULL &amp;&amp; _resproj-&gt;outcnt() != 0) {
1025     // First disconnect stores captured by Initialize node.
1026     // If Initialize node is eliminated first in the following code,
1027     // it will kill such stores and DUIterator_Last will assert.
1028     for (DUIterator_Fast jmax, j = _resproj-&gt;fast_outs(jmax);  j &lt; jmax; j++) {
1029       Node *use = _resproj-&gt;fast_out(j);
1030       if (use-&gt;is_AddP()) {
1031         // raw memory addresses used only by the initialization
1032         _igvn.replace_node(use, C-&gt;top());
1033         --j; --jmax;
1034       }
1035     }
1036     for (DUIterator_Last jmin, j = _resproj-&gt;last_outs(jmin); j &gt;= jmin; ) {
1037       Node *use = _resproj-&gt;last_out(j);
1038       uint oc1 = _resproj-&gt;outcnt();
1039       if (use-&gt;is_Initialize()) {
1040         // Eliminate Initialize node.
</pre>
<hr />
<pre>
1069   if (_fallthroughcatchproj != NULL) {
1070     _igvn.replace_node(_fallthroughcatchproj, alloc-&gt;in(TypeFunc::Control));
1071   }
1072   if (_memproj_fallthrough != NULL) {
1073     _igvn.replace_node(_memproj_fallthrough, alloc-&gt;in(TypeFunc::Memory));
1074   }
1075   if (_memproj_catchall != NULL) {
1076     _igvn.replace_node(_memproj_catchall, C-&gt;top());
1077   }
1078   if (_ioproj_fallthrough != NULL) {
1079     _igvn.replace_node(_ioproj_fallthrough, alloc-&gt;in(TypeFunc::I_O));
1080   }
1081   if (_ioproj_catchall != NULL) {
1082     _igvn.replace_node(_ioproj_catchall, C-&gt;top());
1083   }
1084   if (_catchallcatchproj != NULL) {
1085     _igvn.replace_node(_catchallcatchproj, C-&gt;top());
1086   }
1087 }
1088 


































































































































































































































































































































































































































































































1089 bool PhaseMacroExpand::eliminate_allocate_node(AllocateNode *alloc) {
1090   // Don&#39;t do scalar replacement if the frame can be popped by JVMTI:
1091   // if reallocation fails during deoptimization we&#39;ll pop all
1092   // interpreter frames for this compiled frame and that won&#39;t play
1093   // nice with JVMTI popframe.
1094   if (!EliminateAllocations || JvmtiExport::can_pop_frame() || !alloc-&gt;_is_non_escaping) {
1095     return false;
1096   }
1097   Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
1098   const TypeKlassPtr* tklass = _igvn.type(klass)-&gt;is_klassptr();
1099   Node* res = alloc-&gt;result_cast();
1100   // Eliminate boxing allocations which are not used
1101   // regardless scalar replacable status.
1102   bool boxing_alloc = C-&gt;eliminate_boxing() &amp;&amp;
1103                       tklass-&gt;klass()-&gt;is_instance_klass()  &amp;&amp;
1104                       tklass-&gt;klass()-&gt;as_instance_klass()-&gt;is_box_klass();
1105   if (!alloc-&gt;_is_scalar_replaceable &amp;&amp; (!boxing_alloc || (res != NULL))) {
1106     return false;
1107   }
1108 
</pre>
<hr />
<pre>
1125 
1126   if (!scalar_replacement(alloc, safepoints)) {
1127     return false;
1128   }
1129 
1130   CompileLog* log = C-&gt;log();
1131   if (log != NULL) {
1132     log-&gt;head(&quot;eliminate_allocation type=&#39;%d&#39;&quot;,
1133               log-&gt;identify(tklass-&gt;klass()));
1134     JVMState* p = alloc-&gt;jvms();
1135     while (p != NULL) {
1136       log-&gt;elem(&quot;jvms bci=&#39;%d&#39; method=&#39;%d&#39;&quot;, p-&gt;bci(), log-&gt;identify(p-&gt;method()));
1137       p = p-&gt;caller();
1138     }
1139     log-&gt;tail(&quot;eliminate_allocation&quot;);
1140   }
1141 
1142   process_users_of_allocation(alloc);
1143 
1144 #ifndef PRODUCT
<span class="line-modified">1145   if (PrintEliminateAllocations) {</span>
1146     if (alloc-&gt;is_AllocateArray())
1147       tty-&gt;print_cr(&quot;++++ Eliminated: %d AllocateArray&quot;, alloc-&gt;_idx);
1148     else
1149       tty-&gt;print_cr(&quot;++++ Eliminated: %d Allocate&quot;, alloc-&gt;_idx);
1150   }
1151 #endif
1152 
1153   return true;
1154 }
1155 
1156 bool PhaseMacroExpand::eliminate_boxing_node(CallStaticJavaNode *boxing) {
1157   // EA should remove all uses of non-escaping boxing node.
1158   if (!C-&gt;eliminate_boxing() || boxing-&gt;proj_out_or_null(TypeFunc::Parms) != NULL) {
1159     return false;
1160   }
1161 
1162   assert(boxing-&gt;result_cast() == NULL, &quot;unexpected boxing node result&quot;);
1163 
1164   extract_call_projections(boxing);
1165 
1166   const TypeTuple* r = boxing-&gt;tf()-&gt;range();
1167   assert(r-&gt;cnt() &gt; TypeFunc::Parms, &quot;sanity&quot;);
1168   const TypeInstPtr* t = r-&gt;field_at(TypeFunc::Parms)-&gt;isa_instptr();
1169   assert(t != NULL, &quot;sanity&quot;);
1170 
1171   CompileLog* log = C-&gt;log();
1172   if (log != NULL) {
1173     log-&gt;head(&quot;eliminate_boxing type=&#39;%d&#39;&quot;,
1174               log-&gt;identify(t-&gt;klass()));
1175     JVMState* p = boxing-&gt;jvms();
1176     while (p != NULL) {
1177       log-&gt;elem(&quot;jvms bci=&#39;%d&#39; method=&#39;%d&#39;&quot;, p-&gt;bci(), log-&gt;identify(p-&gt;method()));
1178       p = p-&gt;caller();
1179     }
1180     log-&gt;tail(&quot;eliminate_boxing&quot;);
1181   }
1182 
1183   process_users_of_allocation(boxing);
1184 
1185 #ifndef PRODUCT
<span class="line-modified">1186   if (PrintEliminateAllocations) {</span>
1187     tty-&gt;print(&quot;++++ Eliminated: %d &quot;, boxing-&gt;_idx);
1188     boxing-&gt;method()-&gt;print_short_name(tty);
1189     tty-&gt;cr();
1190   }
1191 #endif
1192 
1193   return true;
1194 }
1195 
1196 //---------------------------set_eden_pointers-------------------------
1197 void PhaseMacroExpand::set_eden_pointers(Node* &amp;eden_top_adr, Node* &amp;eden_end_adr) {
1198   if (UseTLAB) {                // Private allocation: load from TLS
1199     Node* thread = transform_later(new ThreadLocalNode());
1200     int tlab_top_offset = in_bytes(JavaThread::tlab_top_offset());
1201     int tlab_end_offset = in_bytes(JavaThread::tlab_end_offset());
1202     eden_top_adr = basic_plus_adr(top()/*not oop*/, thread, tlab_top_offset);
1203     eden_end_adr = basic_plus_adr(top()/*not oop*/, thread, tlab_end_offset);
1204   } else {                      // Shared allocation: load from globals
1205     CollectedHeap* ch = Universe::heap();
1206     address top_adr = (address)ch-&gt;top_addr();
</pre>
<hr />
<pre>
2762       assert(C-&gt;macro_count() == (old_macro_count - 1), &quot;expansion must have deleted one node from macro list&quot;);
2763       break;
2764     case Node::Class_SubTypeCheck:
2765       expand_subtypecheck_node(n-&gt;as_SubTypeCheck());
2766       assert(C-&gt;macro_count() == (old_macro_count - 1), &quot;expansion must have deleted one node from macro list&quot;);
2767       break;
2768     default:
2769       assert(false, &quot;unknown node type in macro list&quot;);
2770     }
2771     assert(C-&gt;macro_count() &lt; macro_count, &quot;must have deleted a node from macro list&quot;);
2772     if (C-&gt;failing())  return true;
2773 
2774     // Clean up the graph so we&#39;re less likely to hit the maximum node
2775     // limit
2776     _igvn.set_delay_transform(false);
2777     _igvn.optimize();
2778     if (C-&gt;failing())  return true;
2779     _igvn.set_delay_transform(true);
2780   }
2781 




















2782   // All nodes except Allocate nodes are expanded now. There could be
2783   // new optimization opportunities (such as folding newly created
2784   // load from a just allocated object). Run IGVN.
2785 
2786   // expand &quot;macro&quot; nodes
2787   // nodes are removed from the macro list as they are processed
2788   while (C-&gt;macro_count() &gt; 0) {
2789     int macro_count = C-&gt;macro_count();
2790     Node * n = C-&gt;macro_node(macro_count-1);
2791     assert(n-&gt;is_macro(), &quot;only macro nodes expected here&quot;);
2792     if (_igvn.type(n) == Type::TOP || (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0)-&gt;is_top())) {
2793       // node is unreachable, so don&#39;t try to expand it
2794       C-&gt;remove_macro_node(n);
2795       continue;
2796     }
2797     // Make sure expansion will not cause node limit to be exceeded.
2798     // Worst case is a macro node gets expanded into about 200 nodes.
2799     // Allow 50% more for optimization.
2800     if (C-&gt;check_node_count(300, &quot;out of nodes before macro expansion&quot;)) {
2801       return true;
2802     }
2803     switch (n-&gt;class_id()) {
2804     case Node::Class_Allocate:
<span class="line-modified">2805       expand_allocate(n-&gt;as_Allocate());</span>


2806       break;
2807     case Node::Class_AllocateArray:
<span class="line-modified">2808       expand_allocate_array(n-&gt;as_AllocateArray());</span>


2809       break;
2810     default:
2811       assert(false, &quot;unknown node type in macro list&quot;);
2812     }
2813     assert(C-&gt;macro_count() &lt; macro_count, &quot;must have deleted a node from macro list&quot;);
2814     if (C-&gt;failing())  return true;
2815 
2816     // Clean up the graph so we&#39;re less likely to hit the maximum node
2817     // limit
2818     _igvn.set_delay_transform(false);
2819     _igvn.optimize();
2820     if (C-&gt;failing())  return true;
2821     _igvn.set_delay_transform(true);
2822   }
2823 
2824   _igvn.set_delay_transform(false);
2825   return false;
2826 }
</pre>
</td>
<td>
<hr />
<pre>
 286         mem = in-&gt;in(TypeFunc::Memory);
 287       } else {
 288         assert(false, &quot;unexpected projection&quot;);
 289       }
 290     } else if (mem-&gt;is_Store()) {
 291       const TypePtr* atype = mem-&gt;as_Store()-&gt;adr_type();
 292       int adr_idx = phase-&gt;C-&gt;get_alias_index(atype);
 293       if (adr_idx == alias_idx) {
 294         assert(atype-&gt;isa_oopptr(), &quot;address type must be oopptr&quot;);
 295         int adr_offset = atype-&gt;offset();
 296         uint adr_iid = atype-&gt;is_oopptr()-&gt;instance_id();
 297         // Array elements references have the same alias_idx
 298         // but different offset and different instance_id.
 299         if (adr_offset == offset &amp;&amp; adr_iid == alloc-&gt;_idx)
 300           return mem;
 301       } else {
 302         assert(adr_idx == Compile::AliasIdxRaw, &quot;address must match or be raw&quot;);
 303       }
 304       mem = mem-&gt;in(MemNode::Memory);
 305     } else if (mem-&gt;is_ClearArray()) {
<span class="line-added"> 306       intptr_t offset;</span>
<span class="line-added"> 307       AllocateNode* alloc = AllocateNode::Ideal_allocation(mem-&gt;in(3), phase, offset);</span>
<span class="line-added"> 308 </span>
<span class="line-added"> 309       if (alloc == NULL) {</span>
<span class="line-added"> 310         return start_mem;</span>
<span class="line-added"> 311       }</span>
<span class="line-added"> 312 </span>
 313       if (!ClearArrayNode::step_through(&amp;mem, alloc-&gt;_idx, phase)) {
 314         // Can not bypass initialization of the instance
 315         // we are looking.
 316         debug_only(intptr_t offset;)
 317         assert(alloc == AllocateNode::Ideal_allocation(mem-&gt;in(3), phase, offset), &quot;sanity&quot;);
 318         InitializeNode* init = alloc-&gt;as_Allocate()-&gt;initialization();
 319         // We are looking for stored value, return Initialize node
 320         // or memory edge from Allocate node.
 321         if (init != NULL)
 322           return init;
 323         else
 324           return alloc-&gt;in(TypeFunc::Memory); // It will produce zero value (see callers).
 325       }
 326       // Otherwise skip it (the call updated &#39;mem&#39; value).
 327     } else if (mem-&gt;Opcode() == Op_SCMemProj) {
 328       mem = mem-&gt;in(0);
 329       Node* adr = NULL;
 330       if (mem-&gt;is_LoadStore()) {
 331         adr = mem-&gt;in(MemNode::Address);
 332       } else {
</pre>
<hr />
<pre>
 717           if (use-&gt;outcnt() == 1 &amp;&amp; use-&gt;unique_out()-&gt;Opcode() == Op_Return) {
 718             NOT_PRODUCT(fail_eliminate = &quot;Object is return value&quot;;)
 719           } else {
 720             NOT_PRODUCT(fail_eliminate = &quot;Object is referenced by Phi&quot;;)
 721           }
 722           DEBUG_ONLY(disq_node = use;)
 723         } else {
 724           if (use-&gt;Opcode() == Op_Return) {
 725             NOT_PRODUCT(fail_eliminate = &quot;Object is return value&quot;;)
 726           }else {
 727             NOT_PRODUCT(fail_eliminate = &quot;Object is referenced by node&quot;;)
 728           }
 729           DEBUG_ONLY(disq_node = use;)
 730         }
 731         can_eliminate = false;
 732       }
 733     }
 734   }
 735 
 736 #ifndef PRODUCT
<span class="line-modified"> 737   if (print_eliminate_allocations()) {</span>
 738     if (can_eliminate) {
 739       tty-&gt;print(&quot;Scalar &quot;);
 740       if (res == NULL)
 741         alloc-&gt;dump();
 742       else
 743         res-&gt;dump();
 744     } else if (alloc-&gt;_is_scalar_replaceable) {
 745       tty-&gt;print(&quot;NotScalar (%s)&quot;, fail_eliminate);
 746       if (res == NULL)
 747         alloc-&gt;dump();
 748       else
 749         res-&gt;dump();
 750 #ifdef ASSERT
 751       if (disq_node != NULL) {
 752           tty-&gt;print(&quot;  &gt;&gt;&gt;&gt; &quot;);
 753           disq_node-&gt;dump();
 754       }
 755 #endif /*ASSERT*/
 756     }
 757   }
 758 #endif
 759   return can_eliminate;
 760 }
 761 
<span class="line-added"> 762 void PhaseMacroExpand::adjust_safepoint_jvms(SafePointNode* sfpt, Node* res, SafePointScalarObjectNode* sobj) {</span>
<span class="line-added"> 763   JVMState *jvms = sfpt-&gt;jvms();</span>
<span class="line-added"> 764   jvms-&gt;set_endoff(sfpt-&gt;req());</span>
<span class="line-added"> 765 </span>
<span class="line-added"> 766   // Now make a pass over the debug information replacing any references</span>
<span class="line-added"> 767   // to the allocated object with &quot;sobj&quot;</span>
<span class="line-added"> 768   int start = jvms-&gt;debug_start();</span>
<span class="line-added"> 769   int end   = jvms-&gt;debug_end();</span>
<span class="line-added"> 770   sfpt-&gt;replace_edges_in_range(res, sobj, start, end);</span>
<span class="line-added"> 771   _igvn._worklist.push(sfpt);</span>
<span class="line-added"> 772 }</span>
<span class="line-added"> 773 </span>
 774 // Do scalar replacement.
 775 bool PhaseMacroExpand::scalar_replacement(AllocateNode *alloc, GrowableArray &lt;SafePointNode *&gt;&amp; safepoints) {
 776   GrowableArray &lt;SafePointNode *&gt; safepoints_done;
 777 
 778   ciKlass* klass = NULL;
 779   ciInstanceKlass* iklass = NULL;
 780   int nfields = 0;
 781   int array_base = 0;
 782   int element_size = 0;
 783   BasicType basic_elem_type = T_ILLEGAL;
 784   ciType* elem_type = NULL;
 785 
 786   Node* res = alloc-&gt;result_cast();
 787   assert(res == NULL || res-&gt;is_CheckCastPP(), &quot;unexpected AllocateNode result&quot;);
 788   const TypeOopPtr* res_type = NULL;
 789   if (res != NULL) { // Could be NULL when there are no users
 790     res_type = _igvn.type(res)-&gt;isa_oopptr();
 791   }
 792 
 793   if (res != NULL) {
</pre>
<hr />
<pre>
 886           }
 887           JVMState *jvms = sfpt_done-&gt;jvms();
 888           jvms-&gt;set_endoff(sfpt_done-&gt;req());
 889           // Now make a pass over the debug information replacing any references
 890           // to SafePointScalarObjectNode with the allocated object.
 891           int start = jvms-&gt;debug_start();
 892           int end   = jvms-&gt;debug_end();
 893           for (int i = start; i &lt; end; i++) {
 894             if (sfpt_done-&gt;in(i)-&gt;is_SafePointScalarObject()) {
 895               SafePointScalarObjectNode* scobj = sfpt_done-&gt;in(i)-&gt;as_SafePointScalarObject();
 896               if (scobj-&gt;first_index(jvms) == sfpt_done-&gt;req() &amp;&amp;
 897                   scobj-&gt;n_fields() == (uint)nfields) {
 898                 assert(scobj-&gt;alloc() == alloc, &quot;sanity&quot;);
 899                 sfpt_done-&gt;set_req(i, res);
 900               }
 901             }
 902           }
 903           _igvn._worklist.push(sfpt_done);
 904         }
 905 #ifndef PRODUCT
<span class="line-modified"> 906         if (print_eliminate_allocations()) {</span>
 907           if (field != NULL) {
 908             tty-&gt;print(&quot;=== At SafePoint node %d can&#39;t find value of Field: &quot;,
 909                        sfpt-&gt;_idx);
 910             field-&gt;print();
 911             int field_idx = C-&gt;get_alias_index(field_addr_type);
 912             tty-&gt;print(&quot; (alias_idx=%d)&quot;, field_idx);
 913           } else { // Array&#39;s element
 914             tty-&gt;print(&quot;=== At SafePoint node %d can&#39;t find value of array element [%d]&quot;,
 915                        sfpt-&gt;_idx, j);
 916           }
 917           tty-&gt;print(&quot;, which prevents elimination of: &quot;);
 918           if (res == NULL)
 919             alloc-&gt;dump();
 920           else
 921             res-&gt;dump();
 922         }
 923 #endif
 924         return false;
 925       }
 926       if (UseCompressedOops &amp;&amp; field_type-&gt;isa_narrowoop()) {
 927         // Enable &quot;DecodeN(EncodeP(Allocate)) --&gt; Allocate&quot; transformation
 928         // to be able scalar replace the allocation.
 929         if (field_val-&gt;is_EncodeP()) {
 930           field_val = field_val-&gt;in(1);
 931         } else {
 932           field_val = transform_later(new DecodeNNode(field_val, field_val-&gt;get_ptr_type()));
 933         }
 934       }
 935       sfpt-&gt;add_req(field_val);
 936     }
<span class="line-modified"> 937     adjust_safepoint_jvms(sfpt, res, sobj);</span>







 938     safepoints_done.append_if_missing(sfpt); // keep it for rollback
 939   }
 940   return true;
 941 }
 942 
 943 static void disconnect_projections(MultiNode* n, PhaseIterGVN&amp; igvn) {
 944   Node* ctl_proj = n-&gt;proj_out_or_null(TypeFunc::Control);
 945   Node* mem_proj = n-&gt;proj_out_or_null(TypeFunc::Memory);
 946   if (ctl_proj != NULL) {
 947     igvn.replace_node(ctl_proj, n-&gt;in(0));
 948   }
 949   if (mem_proj != NULL) {
 950     igvn.replace_node(mem_proj, n-&gt;in(TypeFunc::Memory));
 951   }
 952 }
 953 
 954 // Process users of eliminated allocation.
 955 void PhaseMacroExpand::process_users_of_allocation(CallNode *alloc) {
 956   Node* res = alloc-&gt;result_cast();
 957   if (res != NULL) {
</pre>
<hr />
<pre>
1013           // Disconnect src right away: it can help find new
1014           // opportunities for allocation elimination
1015           Node* src = ac-&gt;in(ArrayCopyNode::Src);
1016           ac-&gt;replace_edge(src, top());
1017           // src can be top at this point if src and dest of the
1018           // arraycopy were the same
1019           if (src-&gt;outcnt() == 0 &amp;&amp; !src-&gt;is_top()) {
1020             _igvn.remove_dead_node(src);
1021           }
1022         }
1023         _igvn._worklist.push(ac);
1024       } else {
1025         eliminate_gc_barrier(use);
1026       }
1027       j -= (oc1 - res-&gt;outcnt());
1028     }
1029     assert(res-&gt;outcnt() == 0, &quot;all uses of allocated objects must be deleted&quot;);
1030     _igvn.remove_dead_node(res);
1031   }
1032 
<span class="line-added">1033   eliminate_unused_allocation_edges(alloc);</span>
<span class="line-added">1034 }</span>
<span class="line-added">1035 </span>
<span class="line-added">1036 void PhaseMacroExpand::eliminate_unused_allocation_edges(CallNode* alloc) {</span>
1037   //
1038   // Process other users of allocation&#39;s projections
1039   //
1040   if (_resproj != NULL &amp;&amp; _resproj-&gt;outcnt() != 0) {
1041     // First disconnect stores captured by Initialize node.
1042     // If Initialize node is eliminated first in the following code,
1043     // it will kill such stores and DUIterator_Last will assert.
1044     for (DUIterator_Fast jmax, j = _resproj-&gt;fast_outs(jmax);  j &lt; jmax; j++) {
1045       Node *use = _resproj-&gt;fast_out(j);
1046       if (use-&gt;is_AddP()) {
1047         // raw memory addresses used only by the initialization
1048         _igvn.replace_node(use, C-&gt;top());
1049         --j; --jmax;
1050       }
1051     }
1052     for (DUIterator_Last jmin, j = _resproj-&gt;last_outs(jmin); j &gt;= jmin; ) {
1053       Node *use = _resproj-&gt;last_out(j);
1054       uint oc1 = _resproj-&gt;outcnt();
1055       if (use-&gt;is_Initialize()) {
1056         // Eliminate Initialize node.
</pre>
<hr />
<pre>
1085   if (_fallthroughcatchproj != NULL) {
1086     _igvn.replace_node(_fallthroughcatchproj, alloc-&gt;in(TypeFunc::Control));
1087   }
1088   if (_memproj_fallthrough != NULL) {
1089     _igvn.replace_node(_memproj_fallthrough, alloc-&gt;in(TypeFunc::Memory));
1090   }
1091   if (_memproj_catchall != NULL) {
1092     _igvn.replace_node(_memproj_catchall, C-&gt;top());
1093   }
1094   if (_ioproj_fallthrough != NULL) {
1095     _igvn.replace_node(_ioproj_fallthrough, alloc-&gt;in(TypeFunc::I_O));
1096   }
1097   if (_ioproj_catchall != NULL) {
1098     _igvn.replace_node(_ioproj_catchall, C-&gt;top());
1099   }
1100   if (_catchallcatchproj != NULL) {
1101     _igvn.replace_node(_catchallcatchproj, C-&gt;top());
1102   }
1103 }
1104 
<span class="line-added">1105 #define STACK_REG_BUFFER 4</span>
<span class="line-added">1106 </span>
<span class="line-added">1107 bool PhaseMacroExpand::stack_allocation_location_representable(int slot_location) {</span>
<span class="line-added">1108   // TODO This is likely not enough as there are values on the stack above the fixed slots</span>
<span class="line-added">1109   // Revist to see if there is a better check</span>
<span class="line-added">1110   OptoReg::Name stack_reg = OptoReg::stack2reg(slot_location + STACK_REG_BUFFER);</span>
<span class="line-added">1111   if (RegMask::can_represent(stack_reg)) {</span>
<span class="line-added">1112     return true;</span>
<span class="line-added">1113   } else {</span>
<span class="line-added">1114     return false;</span>
<span class="line-added">1115   }</span>
<span class="line-added">1116 }</span>
<span class="line-added">1117 </span>
<span class="line-added">1118 #undef STACK_REG_BUFFER</span>
<span class="line-added">1119 </span>
<span class="line-added">1120 int PhaseMacroExpand::next_stack_allocated_object(int num_slots) {</span>
<span class="line-added">1121   int current = C-&gt;fixed_slots();</span>
<span class="line-added">1122   int next    = current + num_slots;</span>
<span class="line-added">1123   if (!stack_allocation_location_representable(next)) {</span>
<span class="line-added">1124     return -1;</span>
<span class="line-added">1125   }</span>
<span class="line-added">1126   // Keep the toplevel high water mark current:</span>
<span class="line-added">1127   if (C-&gt;fixed_slots() &lt; next) C-&gt;set_fixed_slots(next);</span>
<span class="line-added">1128   return current;</span>
<span class="line-added">1129 }</span>
<span class="line-added">1130 </span>
<span class="line-added">1131 bool PhaseMacroExpand::process_write_barriers_on_stack_allocated_objects(AllocateNode* alloc) {</span>
<span class="line-added">1132   GrowableArray&lt;Node*&gt; barriers;</span>
<span class="line-added">1133   Node *res = alloc-&gt;result_cast();</span>
<span class="line-added">1134   assert(res != NULL, &quot;result node must not be null&quot;);</span>
<span class="line-added">1135 </span>
<span class="line-added">1136   // Find direct barriers on the stack allocated objects.</span>
<span class="line-added">1137   // Those we can simply eliminate.</span>
<span class="line-added">1138   for (DUIterator_Fast imax, i = res-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1139     Node *use = res-&gt;fast_out(i);</span>
<span class="line-added">1140     if (use-&gt;Opcode() == Op_CastP2X) {</span>
<span class="line-added">1141       barriers.append_if_missing(use);</span>
<span class="line-added">1142     } else if (use-&gt;is_AddP()) {</span>
<span class="line-added">1143       for (DUIterator_Fast jmax, j = use-&gt;fast_outs(jmax); j &lt; jmax; j++) {</span>
<span class="line-added">1144         Node *addp_out = use-&gt;fast_out(j);</span>
<span class="line-added">1145         if (addp_out-&gt;Opcode() == Op_CastP2X) {</span>
<span class="line-added">1146           barriers.append_if_missing(addp_out);</span>
<span class="line-added">1147         }</span>
<span class="line-added">1148       }</span>
<span class="line-added">1149     }</span>
<span class="line-added">1150   }</span>
<span class="line-added">1151 </span>
<span class="line-added">1152   while (barriers.length() != 0) {</span>
<span class="line-added">1153     eliminate_gc_barrier(barriers.pop());</span>
<span class="line-added">1154   }</span>
<span class="line-added">1155 </span>
<span class="line-added">1156   // After removing the direct barriers result may no longer be used</span>
<span class="line-added">1157   if (alloc-&gt;result_cast() == NULL) {</span>
<span class="line-added">1158     return true;</span>
<span class="line-added">1159   }</span>
<span class="line-added">1160 </span>
<span class="line-added">1161   // Next walk all uses of the allocate to discover the barriers that</span>
<span class="line-added">1162   // might be reachable from our allocate. If the barrier is reachable</span>
<span class="line-added">1163   // from stack allocated object, we unregister it, so that the check</span>
<span class="line-added">1164   // elimination code doesn&#39;t run on it.</span>
<span class="line-added">1165   VectorSet visited(Thread::current()-&gt;resource_area());</span>
<span class="line-added">1166   GrowableArray&lt;Node*&gt; node_worklist;</span>
<span class="line-added">1167 </span>
<span class="line-added">1168   BarrierSetC2 *bs = BarrierSet::barrier_set()-&gt;barrier_set_c2();</span>
<span class="line-added">1169 </span>
<span class="line-added">1170   node_worklist.push(res);</span>
<span class="line-added">1171 </span>
<span class="line-added">1172   while(node_worklist.length() != 0) {</span>
<span class="line-added">1173     Node* n = node_worklist.pop();</span>
<span class="line-added">1174 </span>
<span class="line-added">1175     if (visited.test_set(n-&gt;_idx)) {</span>
<span class="line-added">1176       continue;  // already processed</span>
<span class="line-added">1177     }</span>
<span class="line-added">1178 </span>
<span class="line-added">1179     for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1180       Node *use = n-&gt;fast_out(i);</span>
<span class="line-added">1181       if (use-&gt;Opcode() == Op_CastP2X) {</span>
<span class="line-added">1182         bs-&gt;unregister_potential_barrier_node(use);</span>
<span class="line-added">1183       } else if (use-&gt;is_Phi() ||</span>
<span class="line-added">1184                  use-&gt;is_CheckCastPP() ||</span>
<span class="line-added">1185                  use-&gt;is_EncodeP() ||</span>
<span class="line-added">1186                  use-&gt;is_DecodeN() ||</span>
<span class="line-added">1187                  use-&gt;is_SafePoint() ||</span>
<span class="line-added">1188                  use-&gt;is_Proj() ||</span>
<span class="line-added">1189                  (use-&gt;is_ConstraintCast() &amp;&amp; use-&gt;Opcode() == Op_CastPP)) {</span>
<span class="line-added">1190         // Find barriers beyond our current result</span>
<span class="line-added">1191         node_worklist.push(use);</span>
<span class="line-added">1192       } else if (use-&gt;is_Store() &amp;&amp; use-&gt;Opcode() == Op_StoreP) {</span>
<span class="line-added">1193         if (n != use-&gt;in(MemNode::ValueIn)) {</span>
<span class="line-added">1194           continue;</span>
<span class="line-added">1195         }</span>
<span class="line-added">1196         // TODO code copied from escape.cpp::ConnectionGraph::get_addp_base.</span>
<span class="line-added">1197         // Common up this code into a helper</span>
<span class="line-added">1198         Node *memory = use-&gt;in(MemNode::Address);</span>
<span class="line-added">1199         if (memory-&gt;is_AddP()) {</span>
<span class="line-added">1200           Node *base = memory-&gt;in(AddPNode::Base);</span>
<span class="line-added">1201           if (base-&gt;uncast()-&gt;is_top()) { // The AddP case #3 and #6 and #9.</span>
<span class="line-added">1202             base = memory-&gt;in(AddPNode::Address);</span>
<span class="line-added">1203             while (base-&gt;is_AddP()) {</span>
<span class="line-added">1204               // Case #6 (unsafe access) may have several chained AddP nodes.</span>
<span class="line-added">1205               assert(base-&gt;in(AddPNode::Base)-&gt;uncast()-&gt;is_top(), &quot;expected unsafe access address only&quot;);</span>
<span class="line-added">1206               base = base-&gt;in(AddPNode::Address);</span>
<span class="line-added">1207             }</span>
<span class="line-added">1208             if (base-&gt;Opcode() == Op_CheckCastPP &amp;&amp;</span>
<span class="line-added">1209                 base-&gt;bottom_type()-&gt;isa_rawptr() &amp;&amp;</span>
<span class="line-added">1210                 _igvn.type(base-&gt;in(1))-&gt;isa_oopptr()) {</span>
<span class="line-added">1211               base = base-&gt;in(1); // Case #9</span>
<span class="line-added">1212             }</span>
<span class="line-added">1213           }</span>
<span class="line-added">1214           node_worklist.push(base);</span>
<span class="line-added">1215         }</span>
<span class="line-added">1216       } else if (use-&gt;is_AddP() ||</span>
<span class="line-added">1217            (use-&gt;is_Load() &amp;&amp; use-&gt;Opcode() == Op_LoadP)) {</span>
<span class="line-added">1218         // Find barriers for loads</span>
<span class="line-added">1219         node_worklist.push(use);</span>
<span class="line-added">1220       }</span>
<span class="line-added">1221     }</span>
<span class="line-added">1222   }</span>
<span class="line-added">1223   return false;</span>
<span class="line-added">1224 }</span>
<span class="line-added">1225 </span>
<span class="line-added">1226 bool PhaseMacroExpand::register_stack_allocated_object_with_safepoints(AllocateNode* alloc, Node* stack_oop) {</span>
<span class="line-added">1227   VectorSet visited(Thread::current()-&gt;resource_area());</span>
<span class="line-added">1228   GrowableArray&lt;Node*&gt; node_worklist;</span>
<span class="line-added">1229   GrowableArray&lt;SafePointNode*&gt; temp;</span>
<span class="line-added">1230   Dict* safepoint_map = new Dict(cmpkey, hashkey);</span>
<span class="line-added">1231   bool found_non_direct_safepoint = false;</span>
<span class="line-added">1232   Node *res = alloc-&gt;result_cast();</span>
<span class="line-added">1233 </span>
<span class="line-added">1234   assert(res != NULL, &quot;result node must not be null&quot;);</span>
<span class="line-added">1235 </span>
<span class="line-added">1236   node_worklist.push(res);</span>
<span class="line-added">1237 </span>
<span class="line-added">1238   while(node_worklist.length() != 0) {</span>
<span class="line-added">1239     Node* n = node_worklist.pop();</span>
<span class="line-added">1240 </span>
<span class="line-added">1241     if (visited.test_set(n-&gt;_idx)) {</span>
<span class="line-added">1242       continue;  // already processed</span>
<span class="line-added">1243     }</span>
<span class="line-added">1244 </span>
<span class="line-added">1245     for (DUIterator_Fast imax, i = n-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1246       Node *use = n-&gt;fast_out(i);</span>
<span class="line-added">1247       if (use-&gt;is_SafePoint()) {</span>
<span class="line-added">1248         SafePointNode* sfpt = use-&gt;as_SafePoint();</span>
<span class="line-added">1249         if (sfpt-&gt;jvms() != NULL) {</span>
<span class="line-added">1250           temp.push(sfpt);</span>
<span class="line-added">1251         }</span>
<span class="line-added">1252       } else if (use-&gt;is_Phi() ||</span>
<span class="line-added">1253           use-&gt;is_CheckCastPP() ||</span>
<span class="line-added">1254           use-&gt;is_EncodeP() ||</span>
<span class="line-added">1255           use-&gt;is_DecodeN() ||</span>
<span class="line-added">1256           use-&gt;is_Proj() ||</span>
<span class="line-added">1257           (use-&gt;Opcode() == Op_CastP2X) ||</span>
<span class="line-added">1258           use-&gt;is_MergeMem() ||</span>
<span class="line-added">1259           use-&gt;is_MemBar() ||</span>
<span class="line-added">1260           (use-&gt;is_ConstraintCast() &amp;&amp; use-&gt;Opcode() == Op_CastPP)) {</span>
<span class="line-added">1261         // Find safepoints beyond our current result</span>
<span class="line-added">1262         node_worklist.push(use);</span>
<span class="line-added">1263       } else if (use-&gt;is_Store() &amp;&amp; use-&gt;Opcode() == Op_StoreP) {</span>
<span class="line-added">1264         node_worklist.push(use);</span>
<span class="line-added">1265         if (n != use-&gt;in(MemNode::ValueIn)) {</span>
<span class="line-added">1266           continue;</span>
<span class="line-added">1267         }</span>
<span class="line-added">1268         // TODO code copied from escape.cpp::ConnectionGraph::get_addp_base.</span>
<span class="line-added">1269         // Common up this code into a helper</span>
<span class="line-added">1270         Node *memory = use-&gt;in(MemNode::Address);</span>
<span class="line-added">1271         if (memory-&gt;is_AddP()) {</span>
<span class="line-added">1272           Node *base = memory-&gt;in(AddPNode::Base);</span>
<span class="line-added">1273           if (base-&gt;uncast()-&gt;is_top()) { // The AddP case #3 and #6 and #9.</span>
<span class="line-added">1274             base = memory-&gt;in(AddPNode::Address);</span>
<span class="line-added">1275             while (base-&gt;is_AddP()) {</span>
<span class="line-added">1276               // Case #6 (unsafe access) may have several chained AddP nodes.</span>
<span class="line-added">1277               assert(base-&gt;in(AddPNode::Base)-&gt;uncast()-&gt;is_top(), &quot;expected unsafe access address only&quot;);</span>
<span class="line-added">1278               base = base-&gt;in(AddPNode::Address);</span>
<span class="line-added">1279             }</span>
<span class="line-added">1280             if (base-&gt;Opcode() == Op_CheckCastPP &amp;&amp;</span>
<span class="line-added">1281                 base-&gt;bottom_type()-&gt;isa_rawptr() &amp;&amp;</span>
<span class="line-added">1282                 _igvn.type(base-&gt;in(1))-&gt;isa_oopptr()) {</span>
<span class="line-added">1283               base = base-&gt;in(1); // Case #9</span>
<span class="line-added">1284             }</span>
<span class="line-added">1285           }</span>
<span class="line-added">1286           node_worklist.push(base);</span>
<span class="line-added">1287         }</span>
<span class="line-added">1288       } else if (use-&gt;is_AddP() ||</span>
<span class="line-added">1289         (use-&gt;is_Load() &amp;&amp; use-&gt;Opcode() == Op_LoadP)) {</span>
<span class="line-added">1290         // Find safepoints for arrays</span>
<span class="line-added">1291         node_worklist.push(use);</span>
<span class="line-added">1292       }</span>
<span class="line-added">1293     }</span>
<span class="line-added">1294 </span>
<span class="line-added">1295     while (temp.length() != 0) {</span>
<span class="line-added">1296       SafePointNode* sfpt = temp.pop();</span>
<span class="line-added">1297       if (res != n) {</span>
<span class="line-added">1298         found_non_direct_safepoint = true;</span>
<span class="line-added">1299       }</span>
<span class="line-added">1300       handle_safepoint_for_stack_allocation(safepoint_map, alloc, stack_oop, n, sfpt);</span>
<span class="line-added">1301     }</span>
<span class="line-added">1302   }</span>
<span class="line-added">1303 </span>
<span class="line-added">1304   return found_non_direct_safepoint;</span>
<span class="line-added">1305 }</span>
<span class="line-added">1306 </span>
<span class="line-added">1307 void PhaseMacroExpand::handle_safepoint_for_stack_allocation(Dict* safepoint_map, AllocateNode* alloc, Node* oop_node, Node* parent, SafePointNode* sfpt) {</span>
<span class="line-added">1308   Node* res = alloc-&gt;result_cast();</span>
<span class="line-added">1309   assert(res-&gt;is_CheckCastPP(), &quot;unexpected AllocateNode result&quot;);</span>
<span class="line-added">1310   const TypeOopPtr* res_type = _igvn.type(res)-&gt;isa_oopptr();</span>
<span class="line-added">1311   ciKlass* klass = res_type-&gt;klass();</span>
<span class="line-added">1312   int nfields = 0;</span>
<span class="line-added">1313   if (res_type-&gt;isa_instptr()) {</span>
<span class="line-added">1314     // find the fields of the class which will be needed for safepoint debug information</span>
<span class="line-added">1315     assert(klass-&gt;is_instance_klass(), &quot;must be an instance klass.&quot;);</span>
<span class="line-added">1316     ciInstanceKlass* iklass = klass-&gt;as_instance_klass();</span>
<span class="line-added">1317     nfields = iklass-&gt;nof_nonstatic_fields();</span>
<span class="line-added">1318   } else {</span>
<span class="line-added">1319     // find the array&#39;s elements which will be needed for safepoint debug information</span>
<span class="line-added">1320     nfields = alloc-&gt;in(AllocateNode::ALength)-&gt;find_int_con(-1);</span>
<span class="line-added">1321   }</span>
<span class="line-added">1322 </span>
<span class="line-added">1323   assert(nfields &gt;= 0, &quot;Sanity&quot;);</span>
<span class="line-added">1324 </span>
<span class="line-added">1325   SafePointScalarObjectNode* sobj = NULL;</span>
<span class="line-added">1326   Node *result = (Node *)(*safepoint_map)[sfpt];</span>
<span class="line-added">1327   if (result != NULL) {</span>
<span class="line-added">1328     assert(result-&gt;is_SafePointScalarObject(), &quot;Has to be a safepointscalarobject&quot;);</span>
<span class="line-added">1329     sobj = result-&gt;as_SafePointScalarObject();</span>
<span class="line-added">1330   } else {</span>
<span class="line-added">1331     //</span>
<span class="line-added">1332     // Process the safepoint uses</span>
<span class="line-added">1333     //</span>
<span class="line-added">1334     Node* mem = sfpt-&gt;memory();</span>
<span class="line-added">1335     Node* ctl = sfpt-&gt;control();</span>
<span class="line-added">1336     assert(sfpt-&gt;jvms() != NULL, &quot;missed JVMS&quot;);</span>
<span class="line-added">1337     // Fields of scalar objs are referenced only at the end</span>
<span class="line-added">1338     // of regular debuginfo at the last (youngest) JVMS.</span>
<span class="line-added">1339     // Record relative start index.</span>
<span class="line-added">1340     uint first_ind = (sfpt-&gt;req() - sfpt-&gt;jvms()-&gt;scloff());</span>
<span class="line-added">1341     sobj = new SafePointScalarObjectNode(res_type,</span>
<span class="line-added">1342 #ifdef ASSERT</span>
<span class="line-added">1343                                                 alloc,</span>
<span class="line-added">1344 #endif</span>
<span class="line-added">1345                                                 first_ind, nfields);</span>
<span class="line-added">1346     sobj-&gt;init_req(0, C-&gt;root());</span>
<span class="line-added">1347     sobj-&gt;add_req(oop_node);</span>
<span class="line-added">1348     transform_later(sobj);</span>
<span class="line-added">1349     sobj-&gt;set_stack_allocated(true);</span>
<span class="line-added">1350 </span>
<span class="line-added">1351     JVMState *jvms = sfpt-&gt;jvms();</span>
<span class="line-added">1352     sfpt-&gt;add_req(sobj);</span>
<span class="line-added">1353     jvms-&gt;set_endoff(sfpt-&gt;req());</span>
<span class="line-added">1354     _igvn._worklist.push(sfpt);</span>
<span class="line-added">1355     safepoint_map-&gt;Insert(sfpt, sobj);</span>
<span class="line-added">1356   }</span>
<span class="line-added">1357 </span>
<span class="line-added">1358   if (parent == res) {</span>
<span class="line-added">1359     adjust_safepoint_jvms(sfpt, parent, sobj);</span>
<span class="line-added">1360   }</span>
<span class="line-added">1361 }</span>
<span class="line-added">1362 </span>
<span class="line-added">1363 bool PhaseMacroExpand::can_stack_allocate(AllocateNode* alloc, Node* res, intptr_t size_of_object) {</span>
<span class="line-added">1364   return ((res != NULL) &amp;&amp; alloc-&gt;_is_stack_allocateable &amp;&amp; (size_of_object != -1) &amp;&amp; should_stack_allocate());</span>
<span class="line-added">1365 }</span>
<span class="line-added">1366 </span>
<span class="line-added">1367 void PhaseMacroExpand::estimate_stack_allocation_size(AllocateNode* alloc) {</span>
<span class="line-added">1368   Node* res                  = alloc-&gt;result_cast();</span>
<span class="line-added">1369   Node* size_in_bytes        = alloc-&gt;in(AllocateNode::AllocSize);</span>
<span class="line-added">1370   intptr_t size_of_object    = _igvn.find_intptr_t_con(size_in_bytes, -1);</span>
<span class="line-added">1371 </span>
<span class="line-added">1372   if (alloc-&gt;_is_scalar_replaceable &amp;&amp; !alloc-&gt;_is_stack_allocateable) {</span>
<span class="line-added">1373     C-&gt;set_fail_stack_allocation_with_references(true);</span>
<span class="line-added">1374     return;</span>
<span class="line-added">1375   }</span>
<span class="line-added">1376 </span>
<span class="line-added">1377   bool can_sa = can_stack_allocate(alloc, res, size_of_object);</span>
<span class="line-added">1378   if (alloc-&gt;_is_stack_allocateable &amp;&amp; !can_sa) {</span>
<span class="line-added">1379     // If we marked the object as SA in EA and now we can not fail</span>
<span class="line-added">1380     C-&gt;set_fail_stack_allocation_with_references(true);</span>
<span class="line-added">1381     return;</span>
<span class="line-added">1382   }</span>
<span class="line-added">1383 </span>
<span class="line-added">1384   if (!alloc-&gt;_is_stack_allocateable) {</span>
<span class="line-added">1385     // If we can not SA because EA said no then no need to count the size</span>
<span class="line-added">1386     return;</span>
<span class="line-added">1387   }</span>
<span class="line-added">1388 </span>
<span class="line-added">1389   int current = C-&gt;stack_allocated_slots();</span>
<span class="line-added">1390   C-&gt;set_stack_allocated_slots(current + (size_of_object &gt;&gt; LogBytesPerInt));</span>
<span class="line-added">1391 }</span>
<span class="line-added">1392 </span>
<span class="line-added">1393 // Do stack allocation</span>
<span class="line-added">1394 bool PhaseMacroExpand::stack_allocation(AllocateNode* alloc) {</span>
<span class="line-added">1395   Node* klass                = alloc-&gt;in(AllocateNode::KlassNode);</span>
<span class="line-added">1396   const TypeKlassPtr* tklass = _igvn.type(klass)-&gt;is_klassptr();</span>
<span class="line-added">1397   Node *length               = (alloc-&gt;is_AllocateArray()) ? alloc-&gt;in(AllocateNode::ALength) : NULL;</span>
<span class="line-added">1398   Node* size_in_bytes        = alloc-&gt;in(AllocateNode::AllocSize);</span>
<span class="line-added">1399   Node* res                  = alloc-&gt;result_cast();</span>
<span class="line-added">1400   Node* ctrl                 = alloc-&gt;in(TypeFunc::Control);</span>
<span class="line-added">1401   Node* mem                  = alloc-&gt;in(TypeFunc::Memory);</span>
<span class="line-added">1402 </span>
<span class="line-added">1403   intptr_t size_of_object = _igvn.find_intptr_t_con(size_in_bytes, -1);</span>
<span class="line-added">1404 </span>
<span class="line-added">1405   if (!can_stack_allocate(alloc, res, size_of_object)) {</span>
<span class="line-added">1406     return false;</span>
<span class="line-added">1407   }</span>
<span class="line-added">1408 </span>
<span class="line-added">1409   if (C-&gt;fail_stack_allocation_with_references()) {</span>
<span class="line-added">1410     if (alloc-&gt;_is_referenced_stack_allocation) {</span>
<span class="line-added">1411 #ifndef PRODUCT</span>
<span class="line-added">1412       if (print_stack_allocation()) {</span>
<span class="line-added">1413         tty-&gt;print_cr(&quot;---- Avoiding stack allocation on node %d because it is referenced by another alloc and SCR/SA failed in method %s&quot;, alloc-&gt;_idx, _igvn.C-&gt;method()-&gt;get_Method()-&gt;name_and_sig_as_C_string());</span>
<span class="line-added">1414       }</span>
<span class="line-added">1415 #endif</span>
<span class="line-added">1416     return false;</span>
<span class="line-added">1417     }</span>
<span class="line-added">1418   }</span>
<span class="line-added">1419 </span>
<span class="line-added">1420   int next_stack_allocation_slot = next_stack_allocated_object(size_of_object &gt;&gt; LogBytesPerInt);</span>
<span class="line-added">1421   if (next_stack_allocation_slot &lt; 0) {</span>
<span class="line-added">1422 #ifndef PRODUCT</span>
<span class="line-added">1423     if (print_stack_allocation()) {</span>
<span class="line-added">1424       tty-&gt;print_cr(&quot;---- Avoiding stack allocation on node %d with size %ld for method %s because of insufficient stack space&quot;, alloc-&gt;_idx, size_of_object, _igvn.C-&gt;method()-&gt;get_Method()-&gt;name_and_sig_as_C_string());</span>
<span class="line-added">1425     }</span>
<span class="line-added">1426 #endif</span>
<span class="line-added">1427     return false;</span>
<span class="line-added">1428   }</span>
<span class="line-added">1429 </span>
<span class="line-added">1430   if (mem-&gt;is_MergeMem()) {</span>
<span class="line-added">1431     mem = mem-&gt;as_MergeMem()-&gt;memory_at(Compile::AliasIdxRaw);</span>
<span class="line-added">1432   }</span>
<span class="line-added">1433 </span>
<span class="line-added">1434   extract_call_projections(alloc);</span>
<span class="line-added">1435 </span>
<span class="line-added">1436   // Process barriers as this may result in result_cast() becoming NULL</span>
<span class="line-added">1437   if (process_write_barriers_on_stack_allocated_objects(alloc)) {</span>
<span class="line-added">1438 #ifndef PRODUCT</span>
<span class="line-added">1439     if (print_stack_allocation()) {</span>
<span class="line-added">1440       tty-&gt;print_cr(&quot;---- Allocation %d result_cast is no longer used so yank the alloc instead&quot;, alloc-&gt;_idx);</span>
<span class="line-added">1441     }</span>
<span class="line-added">1442 #endif</span>
<span class="line-added">1443     InitializeNode* init = alloc-&gt;initialization();</span>
<span class="line-added">1444     if (init != NULL) {</span>
<span class="line-added">1445       init-&gt;remove(&amp;_igvn);</span>
<span class="line-added">1446     }</span>
<span class="line-added">1447     yank_alloc_node(alloc);</span>
<span class="line-added">1448     return true;</span>
<span class="line-added">1449   }</span>
<span class="line-added">1450 </span>
<span class="line-added">1451   assert(res == alloc-&gt;result_cast(), &quot;values much match&quot;);</span>
<span class="line-added">1452 </span>
<span class="line-added">1453   Node* stack_oop = transform_later(new BoxLockNode(next_stack_allocation_slot));</span>
<span class="line-added">1454   Node* new_raw_mem = initialize_object(alloc, ctrl, mem, stack_oop, klass, length, size_in_bytes);</span>
<span class="line-added">1455 </span>
<span class="line-added">1456   bool non_direct_safepoints = register_stack_allocated_object_with_safepoints(alloc, stack_oop);</span>
<span class="line-added">1457   if (non_direct_safepoints) {</span>
<span class="line-added">1458     if (length != NULL) {</span>
<span class="line-added">1459       stack_allocation_init_array_length_on_entry(alloc, length, stack_oop);</span>
<span class="line-added">1460     }</span>
<span class="line-added">1461 #ifndef PRODUCT</span>
<span class="line-added">1462     stack_allocation_clear_object_data(alloc, stack_oop);</span>
<span class="line-added">1463 #endif</span>
<span class="line-added">1464   }</span>
<span class="line-added">1465 </span>
<span class="line-added">1466   _igvn.replace_node(_resproj, stack_oop);</span>
<span class="line-added">1467 </span>
<span class="line-added">1468   for (DUIterator_Fast imax, i = _memproj_fallthrough-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1469     Node *use = _memproj_fallthrough-&gt;fast_out(i);</span>
<span class="line-added">1470     _igvn.rehash_node_delayed(use);</span>
<span class="line-added">1471     imax -= replace_input(use, _memproj_fallthrough, new_raw_mem);</span>
<span class="line-added">1472     // back up iterator</span>
<span class="line-added">1473     --i;</span>
<span class="line-added">1474   }</span>
<span class="line-added">1475 </span>
<span class="line-added">1476   eliminate_unused_allocation_edges(alloc);</span>
<span class="line-added">1477 </span>
<span class="line-added">1478   assert(_resproj-&gt;outcnt() == 0, &quot;all uses of the original allocate result projection must be deleted&quot;);</span>
<span class="line-added">1479   _igvn.remove_dead_node(_resproj);</span>
<span class="line-added">1480 </span>
<span class="line-added">1481 #ifndef PRODUCT</span>
<span class="line-added">1482   if (print_stack_allocation()) {</span>
<span class="line-added">1483     tty-&gt;print_cr(&quot;++++ Performing stack allocation on node %d with size %ld for method %s&quot;, alloc-&gt;_idx, size_of_object, _igvn.C-&gt;method()-&gt;get_Method()-&gt;name_and_sig_as_C_string());</span>
<span class="line-added">1484   }</span>
<span class="line-added">1485 #endif</span>
<span class="line-added">1486 </span>
<span class="line-added">1487   return true;</span>
<span class="line-added">1488 }</span>
<span class="line-added">1489 </span>
<span class="line-added">1490 /*</span>
<span class="line-added">1491   Initialize stack allocated array length on entry to the method.</span>
<span class="line-added">1492   This is required for de-opt so it can verify array lengths and so</span>
<span class="line-added">1493   that GCs that happen after deopt will not crash for uninitialized</span>
<span class="line-added">1494   arrays.</span>
<span class="line-added">1495 */</span>
<span class="line-added">1496 void PhaseMacroExpand::stack_allocation_init_array_length_on_entry(AllocateNode *alloc, Node *length, Node *stack_oop) {</span>
<span class="line-added">1497   Node* start_mem = C-&gt;start()-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-added">1498   assert(length != NULL, &quot;Length can not be NULL&quot;);</span>
<span class="line-added">1499 </span>
<span class="line-added">1500   if (C-&gt;is_osr_compilation()) {</span>
<span class="line-added">1501     for (DUIterator_Fast imax, i = start_mem-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1502       Node *child = start_mem-&gt;fast_out(i);</span>
<span class="line-added">1503       if (child-&gt;is_CallLeaf() &amp;&amp; child-&gt;as_CallLeaf()-&gt;is_call_to_osr_migration_end()) {</span>
<span class="line-added">1504         CallLeafNode* call_leaf = child-&gt;as_CallLeaf();</span>
<span class="line-added">1505         start_mem = call_leaf-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-added">1506         break;</span>
<span class="line-added">1507       }</span>
<span class="line-added">1508     }</span>
<span class="line-added">1509   }</span>
<span class="line-added">1510   assert(start_mem != NULL, &quot;Must find start mem&quot;);</span>
<span class="line-added">1511   Node* init_mem = start_mem;</span>
<span class="line-added">1512 </span>
<span class="line-added">1513   // need to set the length field for arrays for deopt</span>
<span class="line-added">1514   init_mem = make_store(C-&gt;start()-&gt;proj_out_or_null(TypeFunc::Control),</span>
<span class="line-added">1515                         init_mem, stack_oop, arrayOopDesc::length_offset_in_bytes(),</span>
<span class="line-added">1516                         length, T_INT);</span>
<span class="line-added">1517 </span>
<span class="line-added">1518 </span>
<span class="line-added">1519   if (init_mem != start_mem) {</span>
<span class="line-added">1520     for (DUIterator_Fast imax, i = start_mem-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1521       Node *use = start_mem-&gt;fast_out(i);</span>
<span class="line-added">1522       // Compressed refs can make a new store which adjusts the start</span>
<span class="line-added">1523       // offet and it&#39;s sourced by start_mem. Make sure we don&#39;t make cycle.</span>
<span class="line-added">1524       if (use == init_mem || (init_mem-&gt;find_edge(use) &gt;= 0)) {</span>
<span class="line-added">1525         continue;</span>
<span class="line-added">1526       }</span>
<span class="line-added">1527       _igvn.rehash_node_delayed(use);</span>
<span class="line-added">1528       imax -= replace_input(use, start_mem, init_mem);</span>
<span class="line-added">1529       // back up iterator</span>
<span class="line-added">1530       --i;</span>
<span class="line-added">1531     }</span>
<span class="line-added">1532   }</span>
<span class="line-added">1533 }</span>
<span class="line-added">1534 </span>
<span class="line-added">1535 #ifndef PRODUCT</span>
<span class="line-added">1536 /*</span>
<span class="line-added">1537   Initialize SA object on entry to the method to ensure it is initialized</span>
<span class="line-added">1538   before safepoints which may only be reachable through phis and the object</span>
<span class="line-added">1539   may not actually have been initialized.</span>
<span class="line-added">1540 */</span>
<span class="line-added">1541 void PhaseMacroExpand::stack_allocation_clear_object_data(AllocateNode *alloc, Node *stack_oop) {</span>
<span class="line-added">1542   Node* klass                = alloc-&gt;in(AllocateNode::KlassNode);</span>
<span class="line-added">1543   Node *length               = (alloc-&gt;is_AllocateArray()) ? alloc-&gt;in(AllocateNode::ALength) : NULL;</span>
<span class="line-added">1544   Node* size_in_bytes        = alloc-&gt;in(AllocateNode::AllocSize);</span>
<span class="line-added">1545   Node* start_mem            = C-&gt;start()-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-added">1546   if (C-&gt;is_osr_compilation()) {</span>
<span class="line-added">1547     for (DUIterator_Fast imax, i = start_mem-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1548       Node *child = start_mem-&gt;fast_out(i);</span>
<span class="line-added">1549       if (child-&gt;is_CallLeaf() &amp;&amp; child-&gt;as_CallLeaf()-&gt;is_call_to_osr_migration_end()) {</span>
<span class="line-added">1550         CallLeafNode* call_leaf = child-&gt;as_CallLeaf();</span>
<span class="line-added">1551         start_mem = call_leaf-&gt;proj_out_or_null(TypeFunc::Memory);</span>
<span class="line-added">1552         break;</span>
<span class="line-added">1553       }</span>
<span class="line-added">1554     }</span>
<span class="line-added">1555   }</span>
<span class="line-added">1556   assert(start_mem != NULL, &quot;Must find start mem&quot;);</span>
<span class="line-added">1557   int header_size = alloc-&gt;minimum_header_size();</span>
<span class="line-added">1558   Node* init_mem = start_mem;</span>
<span class="line-added">1559   if (length != NULL) {</span>
<span class="line-added">1560     // conservatively small header size:</span>
<span class="line-added">1561     header_size = arrayOopDesc::base_offset_in_bytes(T_BYTE);</span>
<span class="line-added">1562     ciKlass* k = _igvn.type(klass)-&gt;is_klassptr()-&gt;klass();</span>
<span class="line-added">1563     if (k-&gt;is_array_klass()) {   // we know the exact header size in most cases:</span>
<span class="line-added">1564       header_size = Klass::layout_helper_header_size(k-&gt;layout_helper());</span>
<span class="line-added">1565     }</span>
<span class="line-added">1566   }</span>
<span class="line-added">1567   init_mem = ClearArrayNode::clear_memory(C-&gt;start()-&gt;proj_out_or_null(TypeFunc::Control),</span>
<span class="line-added">1568                                           init_mem, stack_oop, header_size, size_in_bytes,</span>
<span class="line-added">1569                                           &amp;_igvn);</span>
<span class="line-added">1570   if (init_mem != start_mem) {</span>
<span class="line-added">1571     for (DUIterator_Fast imax, i = start_mem-&gt;fast_outs(imax); i &lt; imax; i++) {</span>
<span class="line-added">1572       Node *use = start_mem-&gt;fast_out(i);</span>
<span class="line-added">1573       // Compressed refs can make a new store which adjusts the start</span>
<span class="line-added">1574       // offet and it&#39;s sourced by start_mem. Make sure we don&#39;t make cycle.</span>
<span class="line-added">1575       if (use == init_mem || (init_mem-&gt;find_edge(use) &gt;= 0)) {</span>
<span class="line-added">1576         continue;</span>
<span class="line-added">1577       }</span>
<span class="line-added">1578       _igvn.rehash_node_delayed(use);</span>
<span class="line-added">1579       imax -= replace_input(use, start_mem, init_mem);</span>
<span class="line-added">1580       // back up iterator</span>
<span class="line-added">1581       --i;</span>
<span class="line-added">1582     }</span>
<span class="line-added">1583   }</span>
<span class="line-added">1584 }</span>
<span class="line-added">1585 #endif</span>
<span class="line-added">1586 </span>
1587 bool PhaseMacroExpand::eliminate_allocate_node(AllocateNode *alloc) {
1588   // Don&#39;t do scalar replacement if the frame can be popped by JVMTI:
1589   // if reallocation fails during deoptimization we&#39;ll pop all
1590   // interpreter frames for this compiled frame and that won&#39;t play
1591   // nice with JVMTI popframe.
1592   if (!EliminateAllocations || JvmtiExport::can_pop_frame() || !alloc-&gt;_is_non_escaping) {
1593     return false;
1594   }
1595   Node* klass = alloc-&gt;in(AllocateNode::KlassNode);
1596   const TypeKlassPtr* tklass = _igvn.type(klass)-&gt;is_klassptr();
1597   Node* res = alloc-&gt;result_cast();
1598   // Eliminate boxing allocations which are not used
1599   // regardless scalar replacable status.
1600   bool boxing_alloc = C-&gt;eliminate_boxing() &amp;&amp;
1601                       tklass-&gt;klass()-&gt;is_instance_klass()  &amp;&amp;
1602                       tklass-&gt;klass()-&gt;as_instance_klass()-&gt;is_box_klass();
1603   if (!alloc-&gt;_is_scalar_replaceable &amp;&amp; (!boxing_alloc || (res != NULL))) {
1604     return false;
1605   }
1606 
</pre>
<hr />
<pre>
1623 
1624   if (!scalar_replacement(alloc, safepoints)) {
1625     return false;
1626   }
1627 
1628   CompileLog* log = C-&gt;log();
1629   if (log != NULL) {
1630     log-&gt;head(&quot;eliminate_allocation type=&#39;%d&#39;&quot;,
1631               log-&gt;identify(tklass-&gt;klass()));
1632     JVMState* p = alloc-&gt;jvms();
1633     while (p != NULL) {
1634       log-&gt;elem(&quot;jvms bci=&#39;%d&#39; method=&#39;%d&#39;&quot;, p-&gt;bci(), log-&gt;identify(p-&gt;method()));
1635       p = p-&gt;caller();
1636     }
1637     log-&gt;tail(&quot;eliminate_allocation&quot;);
1638   }
1639 
1640   process_users_of_allocation(alloc);
1641 
1642 #ifndef PRODUCT
<span class="line-modified">1643   if (print_eliminate_allocations()) {</span>
1644     if (alloc-&gt;is_AllocateArray())
1645       tty-&gt;print_cr(&quot;++++ Eliminated: %d AllocateArray&quot;, alloc-&gt;_idx);
1646     else
1647       tty-&gt;print_cr(&quot;++++ Eliminated: %d Allocate&quot;, alloc-&gt;_idx);
1648   }
1649 #endif
1650 
1651   return true;
1652 }
1653 
1654 bool PhaseMacroExpand::eliminate_boxing_node(CallStaticJavaNode *boxing) {
1655   // EA should remove all uses of non-escaping boxing node.
1656   if (!C-&gt;eliminate_boxing() || boxing-&gt;proj_out_or_null(TypeFunc::Parms) != NULL) {
1657     return false;
1658   }
1659 
1660   assert(boxing-&gt;result_cast() == NULL, &quot;unexpected boxing node result&quot;);
1661 
1662   extract_call_projections(boxing);
1663 
1664   const TypeTuple* r = boxing-&gt;tf()-&gt;range();
1665   assert(r-&gt;cnt() &gt; TypeFunc::Parms, &quot;sanity&quot;);
1666   const TypeInstPtr* t = r-&gt;field_at(TypeFunc::Parms)-&gt;isa_instptr();
1667   assert(t != NULL, &quot;sanity&quot;);
1668 
1669   CompileLog* log = C-&gt;log();
1670   if (log != NULL) {
1671     log-&gt;head(&quot;eliminate_boxing type=&#39;%d&#39;&quot;,
1672               log-&gt;identify(t-&gt;klass()));
1673     JVMState* p = boxing-&gt;jvms();
1674     while (p != NULL) {
1675       log-&gt;elem(&quot;jvms bci=&#39;%d&#39; method=&#39;%d&#39;&quot;, p-&gt;bci(), log-&gt;identify(p-&gt;method()));
1676       p = p-&gt;caller();
1677     }
1678     log-&gt;tail(&quot;eliminate_boxing&quot;);
1679   }
1680 
1681   process_users_of_allocation(boxing);
1682 
1683 #ifndef PRODUCT
<span class="line-modified">1684   if (print_eliminate_allocations()) {</span>
1685     tty-&gt;print(&quot;++++ Eliminated: %d &quot;, boxing-&gt;_idx);
1686     boxing-&gt;method()-&gt;print_short_name(tty);
1687     tty-&gt;cr();
1688   }
1689 #endif
1690 
1691   return true;
1692 }
1693 
1694 //---------------------------set_eden_pointers-------------------------
1695 void PhaseMacroExpand::set_eden_pointers(Node* &amp;eden_top_adr, Node* &amp;eden_end_adr) {
1696   if (UseTLAB) {                // Private allocation: load from TLS
1697     Node* thread = transform_later(new ThreadLocalNode());
1698     int tlab_top_offset = in_bytes(JavaThread::tlab_top_offset());
1699     int tlab_end_offset = in_bytes(JavaThread::tlab_end_offset());
1700     eden_top_adr = basic_plus_adr(top()/*not oop*/, thread, tlab_top_offset);
1701     eden_end_adr = basic_plus_adr(top()/*not oop*/, thread, tlab_end_offset);
1702   } else {                      // Shared allocation: load from globals
1703     CollectedHeap* ch = Universe::heap();
1704     address top_adr = (address)ch-&gt;top_addr();
</pre>
<hr />
<pre>
3260       assert(C-&gt;macro_count() == (old_macro_count - 1), &quot;expansion must have deleted one node from macro list&quot;);
3261       break;
3262     case Node::Class_SubTypeCheck:
3263       expand_subtypecheck_node(n-&gt;as_SubTypeCheck());
3264       assert(C-&gt;macro_count() == (old_macro_count - 1), &quot;expansion must have deleted one node from macro list&quot;);
3265       break;
3266     default:
3267       assert(false, &quot;unknown node type in macro list&quot;);
3268     }
3269     assert(C-&gt;macro_count() &lt; macro_count, &quot;must have deleted a node from macro list&quot;);
3270     if (C-&gt;failing())  return true;
3271 
3272     // Clean up the graph so we&#39;re less likely to hit the maximum node
3273     // limit
3274     _igvn.set_delay_transform(false);
3275     _igvn.optimize();
3276     if (C-&gt;failing())  return true;
3277     _igvn.set_delay_transform(true);
3278   }
3279 
<span class="line-added">3280   for (int i = C-&gt;macro_count(); i &gt; 0; i --) {</span>
<span class="line-added">3281     Node * n = C-&gt;macro_node(i-1);</span>
<span class="line-added">3282     assert(n-&gt;is_macro(), &quot;only macro nodes expected here&quot;);</span>
<span class="line-added">3283 </span>
<span class="line-added">3284     switch (n-&gt;class_id()) {</span>
<span class="line-added">3285     case Node::Class_Allocate:</span>
<span class="line-added">3286     case Node::Class_AllocateArray:</span>
<span class="line-added">3287       estimate_stack_allocation_size(n-&gt;as_Allocate());</span>
<span class="line-added">3288       break;</span>
<span class="line-added">3289     default:</span>
<span class="line-added">3290       assert(false, &quot;unknown node type in macro list&quot;);</span>
<span class="line-added">3291     }</span>
<span class="line-added">3292   }</span>
<span class="line-added">3293 </span>
<span class="line-added">3294   // Check to see if stack allocation size is too large before macro expansion</span>
<span class="line-added">3295   // so we can reject required stack allocations</span>
<span class="line-added">3296   if (!stack_allocation_location_representable(C-&gt;fixed_slots() + C-&gt;stack_allocated_slots())) {</span>
<span class="line-added">3297     C-&gt;set_fail_stack_allocation_with_references(true);</span>
<span class="line-added">3298   }</span>
<span class="line-added">3299 </span>
3300   // All nodes except Allocate nodes are expanded now. There could be
3301   // new optimization opportunities (such as folding newly created
3302   // load from a just allocated object). Run IGVN.
3303 
3304   // expand &quot;macro&quot; nodes
3305   // nodes are removed from the macro list as they are processed
3306   while (C-&gt;macro_count() &gt; 0) {
3307     int macro_count = C-&gt;macro_count();
3308     Node * n = C-&gt;macro_node(macro_count-1);
3309     assert(n-&gt;is_macro(), &quot;only macro nodes expected here&quot;);
3310     if (_igvn.type(n) == Type::TOP || (n-&gt;in(0) != NULL &amp;&amp; n-&gt;in(0)-&gt;is_top())) {
3311       // node is unreachable, so don&#39;t try to expand it
3312       C-&gt;remove_macro_node(n);
3313       continue;
3314     }
3315     // Make sure expansion will not cause node limit to be exceeded.
3316     // Worst case is a macro node gets expanded into about 200 nodes.
3317     // Allow 50% more for optimization.
3318     if (C-&gt;check_node_count(300, &quot;out of nodes before macro expansion&quot;)) {
3319       return true;
3320     }
3321     switch (n-&gt;class_id()) {
3322     case Node::Class_Allocate:
<span class="line-modified">3323       if (!stack_allocation(n-&gt;as_Allocate())) {</span>
<span class="line-added">3324         expand_allocate(n-&gt;as_Allocate());</span>
<span class="line-added">3325       }</span>
3326       break;
3327     case Node::Class_AllocateArray:
<span class="line-modified">3328       if (!stack_allocation(n-&gt;as_AllocateArray())) {</span>
<span class="line-added">3329         expand_allocate_array(n-&gt;as_AllocateArray());</span>
<span class="line-added">3330       }</span>
3331       break;
3332     default:
3333       assert(false, &quot;unknown node type in macro list&quot;);
3334     }
3335     assert(C-&gt;macro_count() &lt; macro_count, &quot;must have deleted a node from macro list&quot;);
3336     if (C-&gt;failing())  return true;
3337 
3338     // Clean up the graph so we&#39;re less likely to hit the maximum node
3339     // limit
3340     _igvn.set_delay_transform(false);
3341     _igvn.optimize();
3342     if (C-&gt;failing())  return true;
3343     _igvn.set_delay_transform(true);
3344   }
3345 
3346   _igvn.set_delay_transform(false);
3347   return false;
3348 }
</pre>
</td>
</tr>
</table>
<center><a href="machnode.hpp.sdiff.html" target="_top">&lt; prev</a> <a href="../../../../index.html" target="_top">index</a> <a href="macro.hpp.sdiff.html" target="_top">next &gt;</a></center>  </body>
</html>